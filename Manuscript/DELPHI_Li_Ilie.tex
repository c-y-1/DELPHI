\documentclass{bioinfo}
\copyrightyear{2015} \pubyear{2015}
\usepackage{url}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{float}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{Manuscript Category}

\begin{document}
\firstpage{1}

\subtitle{Sequence analysis}

\title[Protein interaction sites prediction]{DELPHI: accurate deep ensemble model for protein interaction sites prediction}
\author[Li and Ilie]{Yiwei Li and Lucian Ilie\,$^*$}
\address{Department of Computer Science, The University of Western Ontario, 
London, ON, N6A 5B7, Canada}

\corresp{$^\ast$To whom correspondence should be addressed: \texttt{ilie@uwo.ca}}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}

\abstract{\textbf{Motivation:} Proteins usually perform their functions by interacting with other proteins, which is why accurately predicting protein-protein interaction (PPI) binding sites is a fundamental problem. Experimental methods are slow and expensive. Therefore, great efforts are being made towards increasing the performance of computational methods.\\
\textbf{Results:} We propose DELPHI (DEep Learning Prediction of Highly probable protein Interaction sites), a new sequence-based deep learning suite for PPI binding sites prediction. DELPHI has an ensemble structure with data augmentation and it employs novel features in addition to existing ones. We comprehensively compare DELPHI to nine state-of-the-art programs on five datasets and show that it is more accurate. \\
\textbf{Availability:} The trained model, source code for training, predicting, and data processing are freely available at \texttt{https://github.com/lucian-ilie/DELPHI}. 
All datasets used in this study can be downloaded at
\texttt{http://www.csd.uwo.ca/\~{}ilie/DELPHI/}.\\
\textbf{Contact:} \href{ilie@uwo.ca}{ilie@uwo.ca}\\
\textbf{Supplementary information:} Supplementary data are available at \textit{Bioinformatics}
online.}

\maketitle

\section{Introduction}
Protein-protein interactions (PPI) play a key role in many cellular processes such as signal transduction, transport and metabolism \citep{zhang2018review}. Proteins interact by forming chemical bonds with other proteins. The bonding amino acid residues are protein-protein interaction binding sites. Detecting PPI binding sites helps understand cell regulatory mechanisms, locating drug target, predicting protein functions \citep{bonetta2010interactome}. Databases like PDB \citep{berman2002protein} store protein binding sites information deriving from the 3D structure of each protein, but the available protein structures are limited. Experimental methods such as two-hybrid assay and affinity systems are usually time and labour intensive \citep{shoemaker2007deciphering}. Computational methods are needed to bridge the gap, and many have been developed \cite{cao2006enhanced, ofran2007isis, du2009improved, chen2009sequence, london2010structural, chen2010sequence, murakami2010applying, xue2011homppi, amos2011binding, jones2012psicov, asadabadi2013predictions, singh2014springs, wang2014fast, geng2015prediction, laine2015local, hwang2016hybrid, maheshwari2015prediction, liu2016prediction, wei2016protein, maheshwari2016template, jia2016ippbs, zhang2019sequence, wang2019protein, zhang2019scriber, zeng2019protein, xie2020prediction}. Out of the above mentioned twenty six computational methods, all but one are machine learning based. Computational methods can be classified into three categories, sequenced based, structure based, and combined. Among them, sequence-based approaches are usually faster and cheaper. They are also more universal because comparing to sequence information, structure information is still limited. 

Machine learning methods use feature groups to represent each protein sequence. Widely used features such as position-specific scoring matrix (PSSM), evolutionary conservation (ECO), putative relative solvent accessibility (RSA) have been assessed in \citep{zhang2019comprehensive}. High-scoring segment pair (HSP) has been used in previous methods for PPI prediction \citep{li2017sprint}. One-hot vectors \citep{zhang2019sequence, zeng2019protein} and amino acid embedding \citep{asgari2015continuous, heinzinger2019modeling, asgari2019probabilistic} have also been empirically explored to represent protein sequences.

The learning structure is crucial to PPI binding sites classification problems. Previously explored architectures include random forest \citep{wei2016protein, wang2019protein}, SVM \citep{wei2016protein}, logistic regression \citep{zhang2019scriber}, Bayes classifier \citep{murakami2010applying}, artificial neural networks \citep{singh2014springs}. Recently, convolutional neural network (CNN) \citep{zeng2019protein} and recurrent neural network (RNN) \citep{zhang2019sequence} have also been applied to solve this problem. 

We introduce a new sequence-based PPI binding sites prediction method, DELPHI (DEep Learning Prediction of Highly probable protein Interaction sites), that combines a CNN and a RNN structure. It uses twelve feature groups to represent protein sequences including three novel features, high-scoring segment pair (HSP), position information, and a reduced 3-mer amino acid embedding (ProtVec1D).

We have comprehensively compared DELPHI with nine state-of-the-art programs on five datasets. DELPHI provides the best predictions in all metrics. The contributions of the DELPHI study are as follows.
First, a novel fine tuned ensemble model combing CNN and RNN is constructed in which three features are used the first time in this problem. Second, a many-to-one structure, that not only serves as a data augmentation technique but also improves the prediction performance, is applied. Third, a data processing and feature construction suite, including new features, is provided, aiming to alleviating the difficulty of tedious feature computation by the users.

\begin{methods}
\section{Materials and Methods}
\subsection{Datasets}
\subsubsection{Training and validation datasets}
A large, high quality dataset was provided in \citep{zhang2019comprehensive}. In this dataset, Uniprot sequences are annotated with Protein, DNA, RNA, and small ligands binding information at the residue level. We further processed this dataset as follows. First, we kept only the sequences with protein-protein binding information to focus on protein-protein binding. Then we removed any sequences from training dataset sharing more than 25\% similarities, as measured by PSI-CD-hit \citep{li2006cd,fu2012cd}, with any sequences in testing datasets. It is well acknowledged that similar sequences between training and testing datasets negatively affect the generalization of the evaluated performance of a machine learning model. Also, proteins with higher levels of similarity can be accurately predicted by the alignment-based methods \citep{zhang2018review}. The similarity threshold is picked differently by different programs ranging from 25\% to 50\%. We picked the strictest value of 25\% to match to one of the closest competing programs, SCRIBER \citep{zhang2019scriber}, for a fair comparison. We used PSI-CD-HIT because it is fast, accurate and well maintained in the CD-HIT suite. Also, it is able to cluster sequences with similarity at low as 25\%, whereas CD-HIT works only down to 40\%. Finally, we ran PSI-CD-hit again on the rest of the training protein sequences so no sequences shared more than 25\% similarities among training data. This ensures the training data is as diverse as possible. A dataset of 9,982 protein sequences was constructed. From it, we randomly pick eight ninth (8,872) as the training dataset and one ninth (1,110) as the validation dataset.

\subsubsection{Testing datasets}\label{testing_data}
Five datasets are used in the comparative assessment. Four of them are publicly available dataset from previous studies: Dset\_186 \citep{murakami2010applying}, Dset\_72 \citep{hwang2008protein}, Dset\_164 \citep{dhole2014sequence}, and Dset\_448 \citep{zhang2019scriber}. The first three have been widely used and explored by previous studies while Dset\_448 is more recent. The raw data of Dset\_448 was from the BioLip database \citep{yang2012biolip} where binding sites are defined if the distance between an atom of a residues and an atom of a given protein partner <0.5 \AA{} plus the sum of the Van der Waals radii of the two atoms. The raw data was further processed by the authors of \citep{zhang2019scriber}, removing protein fragments, mapping BioLip sequences to UniProt sequences, clustering so that no similarities above 25\% is shared within the dataset. The number of sequences, binding, non-binding, and the ratio of binding residues are shown in Table \ref{tab_dataset}. The training dataset of DLPred has some overlaps with Dset\_448, therefore we removed 93 proteins in Dset\_448 that share more than 40\% similarities with DLPred training data and formed a reduced testing dataset of 355 proteins (Dset\_355).  According to the PSI-CD-HIT results, the training datasets of DLPred and SCRIBER still contain some proteins that share more than 25\% similarity to Dset\_186,  Dset\_72, and Dset\_164. However, we kept these testing datasets untouched because otherwise we would have to remove too many proteins from them. The training datasets of the competing programs can not be changed because the models are pre-trained. Note that all testing data share less than 25\% similarity to the training dataset of DELPHI. 

\begin{table*}[htbp]
    \centering
    \caption{The datasets used for training, validation, and testing. The first column gives the dataset names. The second column contains the number of proteins in each dataset. The third, fourth, and fifth columns represent the total number of residue, the number of binding, and the number of non-binding residues in each dataset. The last column represents the percentage of the binding residues out of total.}
    \begin{tabular}{p{12em}rrrrr}
    \toprule
    Dataset & Proteins & \multicolumn{3}{c}{Residues} & \multicolumn{1}{c}{\% binding} \\ \cline{3-5}
    & & total & binding & non-binding & \multicolumn{1}{c}{out of total}\\ \hline
    Dset\_448 & 448   & 116,500 & 15,810 & 100,690 & 13.57\% \\
    Dset\_355 & 355   & 95,940 & 11,467 & 84,473 & 11.95\% \\
    Dset\_186 & 186   & 36,219 & 5,517 & 30,702 & 15.23\% \\
    Dset\_72 & 72    & 18,140 & 1,923 & 16,217 & 10.60\% \\
    Dset\_164 & 164   & 33,681 & 6,096 & 27,585 & 18.10\% \\
    Training + validation & 9,982 & 4,254,198 & 427,687 & 3,826,511 & 10.05\% \\
    \hline
    \end{tabular}%
    \label{tab_dataset}%
\end{table*}%

\subsection{Input features}
DELPHI uses 12 features groups, shown in Table \ref{tab_feture}, including high-scoring segment pair (HSP), a variation of 3-mer amino acid embedding (ProtVec1D), position information, position-specific scoring matrix (PSSM), evolutionary conservation (ECO), putative relative solvent accessibility (RSA), relative amino acid propensity (RAA), putative protein-binding disorder, hydropathy index, physicochemical characteristics, physical properties, and PKx. Each input is represented by a 39 dimensional feature vector profile. To the best of our knowledge, this study is the first time that HSP and ProtVec1D are being used in binding sites classification problems. The computation of each of these two new features is described next.

\begin{table*}[htbp]
  \centering
  \caption{The feature groups used by DELPHI. The first column indicates the name of each feature. The second column describes the program used to obtain the feature. ``Load'' means the value for a specific amino acid is known from previous work, and it is loaded in the DELPHI program. ``Compute'' means DELPHI performs additional computation to that feature. The last column shows the dimension of each feature group. Full details are given in the text.}
    \begin{tabular}{p{23.93em}p{14.145em}r}
    \toprule
    Feature & Program & \multicolumn{1}{p{5.145em}}{Dimension} \\
    \midrule
    High-scoring segment pair (HSP) & SPRINT and compute & 1 \\
    % \midrule
    3-mer amino acid embedding (ProtVec1D) & Load and compute & 1 \\
    % \midrule
    Position information & Compute & 1 \\
    Position-specific scoring matrix (PSSM) & Psi-Blast & 20 \\
    % \midrule
    Evolutionary conservation (ECO) & Hhblits & 1 \\
    % \midrule
    Putative relative solvent accessibility (RSA) & ASAquick & 1 \\
    % \midrule
    Relative amino acid propensity (RAA) & Load  & 1 \\
    % \midrule
    Putative protein-binding disorder & ANCHOR & 1 \\
    % \midrule
    Hydropathy index & Load  & 1 \\
    % \midrule
    Physicochemical characteristics & Load  & 3 \\
    % \midrule
    Physical properties & Load  & 7 \\
    % \midrule
    PKx   & Load  & 1 \\
    \bottomrule
    \end{tabular}%
  \label{tab_feture}%
\end{table*}%

High-scoring segment pair (HSP): An HSP is a pair of similar sub-sequences between two proteins. The similarities between two sub-sequence of the same length are measured by scoring matrices such as PAM and BLOSUM. SPRINT \citep{li2017sprint} is used for computing all HSPs as it detects similarities fast and accurately among all proteins in training and testing. After obtaining the HSPs, the score for the $i$th residue, $P[i]$, of a testing protein $P$, denoted $\text{HSP}_{\text{score}}(P[i])$, is calculated as follows. Assume we have an HSP, $(u,v)$, between $P$ and a training protein $Q$ such that $u$ covers the residue $P[i]$, that is, position $i$ in $P$ is within the range covered by $u$. Let $j$ be the position in $Q$ that corresponds to $i$, that is, the distance in $P$ from the beginning of $u$ to $i$ is the same as the distance in $Q$ from the beginning of $v$ to $j$. If $Q[j]$ is a known interacting residue, then we add the PAM120 score between $P[i]$ and $Q[j]$ to the HSP score of $P[i]$:
\[
\text{HSP}_{\text{score}}(P[i]) = \!\!\!\!\!\!\sum_{\stackrel{\text{\tiny HSPs covering $P[i]$}}{\text{\tiny $Q[j]$ interacting residue}}}\!\!\!\!\!\! \max(0, \text{PAM120}(P[i], Q[j])) \ .
\]
The 3-mer amino acid embedding (ProtVec1D): We developed this feature based on ProtVec \citep{asgari2015continuous}. ProtVec uses word2vec \citep{mikolov2013distributed} to construct a one hundred dimensional embedding for each amino acid 3-mer. It is shown in \citep{asgari2015continuous} that ProtVec can be applied to problems such as protein family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. Since using the ProtVec embedding in our program slows down significantly the deep learning model, especially during training, we replaced the one hundred dimensional vector by one dimensional value, which is the sum of the one hundred components; we call this ProtVec1D. According to our tests, ProtVec1D achieves, in connection with the other feaures, the same prediction performance as ProtVec.

Position information: In natural language processing tasks, position information is shown useful. The popular network Bert \citep{devlin2018bert} utilizes this information to guide its translation process. It is also shown by DeepPPISP \citep{zeng2019protein} that the global information of a protein helps the prediction of interfaces. Inspired by the two networks, we use the position information of each amino acid as an input feature hoping that it provides certain global information of a protein. The position of an amino acid in a protein is in the range of 1 to the length of the protein. Then the position is divided by protein's length so that the value is between 0 to 1.

Position-specific scoring matrix (PSSM): PSSM matrices are widely used in protein interaction related problems. They contain the evolutionary conservation of each amino acid position by aligning an input sequence with protein databases. The PSSM matrices are computed using PSI-Blast \citep{altschul1997gapped} with the expectation value (E-value) set to 0.001 and the number of iterations set to 3. PSI-Blast performs multiple alignment on each input sequence against the non-redundant database. 

Evolutionary conservation (ECO): ECO also contains evolutionary conservation, but in a more compact way. To compute the ECO score, the faster multiple alignment tool HHBlits \citep{remmert2012hhblits} is run against the non-redundant Uniprot20 database with default parameters. The one dimensional conservation value is computed using the formula described in \citep{zhang2019comprehensive}.

Putative relative solvent accessibility (RSA): The solvent accessibility is predicted using ASAquick \citep{faraggi2014accurate}. The values are obtained in the from rasaq.pred file in each output directory.

Relative amino acid propensity (RAA): The AA propensity for binding is quantified as relative difference in abundance of a given amino acid type between binding residues and the corresponding non-binding residues located on the protein surface. The RAA for each amino acid type is computed in \citep{zhang2019comprehensive} by using the program of  \citep{vacic2007composition}.

Putative protein-binding disorder: The putative protein-binding disorder is computed using the ANCHOR program \citep{dosztanyi2009anchor}.

Hydropathy index: Hydrophobicity scales is experimentally determined transfer free energies for each amino acid. It contains energetics information of protein-bilayer interactions \citep{wimley1996experimentally}. The values are computed in \citep{kyte1982simple}.

Physicochemical characteristics: For each protein, this includes three features: the number of atoms, electrostatic charges and potential hydrogen bonds for each amino acid. They are taken from \citep{zhang2019sequence}.

Physical properties: We use a 7-dimensional property of each amino acid type. They are a steric parameter (graph-shape index), polarizability, volume (normalized van der Waals volume), hydrophobicity, isoelectric point, helix probability and sheet probability. The pre-computed values are taken from \citep{zhang2019sequence}.

PKx: This is the negative of the logarithm of the dissociation constant for any other group in the molecule. The values for each amino acid type is taken from \citep{zhang2019sequence}.

After computing all the feature vectors, the values in in each row vector are normalized to a number between 0 to 1 using formula (\ref{eq_normalized}) where \textit{v} is the original feature value, and max and min are the biggest and smallest value observed in the training dataset, resp. This is to ensure each feature group are of the same numeric scale and help the model converge better:

\begin{equation}
v_\text{norm}=\dfrac{v-\text{min}}{\text{max}-\text{min}}\label{eq_normalized}
\end{equation}

\subsection{Model architecture}
DELPHI has an architecture that is inspired by ensemble learning. The intuition of the design is that different components of the model capture different information, and another deep neural network is trained to only select the most useful ones. As shown in Fig. \ref{fig_architecture}, the model consists of three parts, a convolutional neural network (CNN) component, an recurrent neural network (RNN) component, and an ensemble component. The core layers of the CNN and RNN components are convolution and bidirectional gated recurrent units (GRU) layers. The ensemble model decodes the output of the first two components.  
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{Model_architecture.pdf}
  \caption{\textbf{The architecture of DELPHI.} Left: the CNN component of the model. Middle: the RNN component of the model. Right: The ensemble model. 
  \label{fig_architecture}}
\end{figure*}

Another very useful characteristic of the model is its many-to-one structure, meaning that the information of many residues are used to prediction the binding propensity of the centered single residue. As illustrated in Fig. \ref{fig_many2one}, for each amino acid as the prediction target, a window of size 31, centred on the amino acid position, is used to collect information from the neighbouring 30 residues to help the prediction. A sliding window is used to capture each 31-mer. The size 31 is determined experimentally. The beginning and the ending part of the sequence are padded with zeros. The many-to-one structure has two advantages. Firstly, it serves as a data augmentation technique. Deep learning models need large amount of data to train, and comparing to image classifiers, models in proteomics have access to orders of magnitude less data. Using each residue multiple times during the training process helps the model learn better. Secondly, it makes the model more robust. The lengths of protein sequence vary from less than one hundred to several thousand, and most a many-to-many models have a fixed input length of near 500. During training, sequences around length 500 are often picked. However, during testing, input sequences are random and need to be either padded or cut into pieces. The different average lengths between training and testing could potentially make the model less general. 
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{many_2_one.pdf}
  \caption{\textbf{The many-to-one prediction.} Sliding windows of size 31, stride 1 are put on top of an input protein sequence. Each time, a sub-sequence of length 31 is extracted. The model predicts the protein-binding propensity of the middle amino acid for each sub-sequence.
  \label{fig_many2one}}
\end{figure*}

\subsubsection{Architecture of the CNN network}
The CNN model one has a concise structure: one convolution layer, one maxpooling layer, one flatten layer, and two fully connected layers. For each input sub-sequence of size 31, a 2D feature profile of size 39 by 31 is constructed. The 2D vector is reshaped into 3D and then passed to a convolution 2D layer, followed by a maxpooling layer. The intuition of using the convolution and maxpooling layers is that a 2D protein profile vector can be considered as an image with one channel, and the CNN model captures the combination of several features in a partial image. The results are flattened and then fed into two fully connected layers with dropout for regularization. The last fully connected layer has one unit with activation function sigmoid, so that the output is a single value between 0 to 1. The higher the value, the more confident the CNN model claims that the residue is a PPI binding site.

\subsubsection{Architecture of the RNN network}
The RNN component has the following structure: one bidirectional GRU layer, one flatten layer, and two fully connected layers. Similar to the CNN component, a 2D feature profile of size 39 by 31 is built for each 31-mer. The feature profile is passed to a bidirectional gated recurrent units (GRU) layer with the intention to memories the dependency and relationship among the 31 residues. We set the GRU layer to return a whole sequence as opposed to return a single value. The results are flattened and fed into two fully connected layers with dropout. The output of the RNN network is also a single value between 0 to 1.

\subsubsection{Architecture of the ensemble network}
The final model combines the core layers of the above mentioned CNN and RNN models and tries to further extract essential information of protein binding. The ensemble network takes a sequence of length 31 as its input. Similar to the CNN and RNN components, a 39 by 31 feature vector is constructed and passed to both a convolution layer and a bidirectional GRU layer. The output of the convolution layer is passed on to a maxpool layer and then flattened. The GRU output is also flattened. Then the outputs of the two flatten layers are concatenated and passed on to two fully connected layers with dropout. The last fully connected layer has one output unit with a sigmoid activation function, so the final output is a single value between 0 to 1, indicating the propensity of being binding sites. This is the final output of the entire model. 

Fine tuning is used in this ensemble model. The convolution layer in the CNN network and the bidirectional GRU layer in the RNN network are tuned separated using the same training/validation dataset. After achieving the best performance on the CNN and the RNN components, the weights of the convolution and the GRU layer are saved to files. In the ensemble model, the convolution and the GRU layer load the saved weights from the file and freeze the weights, so that during the process of training, the convolution and the GRU layer stay unchanged. Training and validation data are used again only to train the fully connected layers in the ensemble model.

\subsection{Implementation}
The program is written in Keras \citep{chollet2015keras} with TensorFlow GPU \citep{tensorflow2015-whitepaper} back end. All features are computed from sequence only. We alleviate the burden of feature computation from users by providing all computation programs and a pipeline script. We ease the system configuration process by providing users a pip package list which enables one-command installation. 

Classifying protein binding residue is an imbalanced problem. To cope with that, different class weights \citep{ting2002instance} are assigned to the positive and negative samples, so that the model pays more attention to the minority class, which is the binding sites.  The values are determined by the inverse of the class distribution in the training datasets. In our program, the weights are 0.55 and 4.97 for the non-binding and binding sites respectively. 

During training, we shuffle the data before each epoch. Since the sliding window is used to extract each 31-mer, adjacent data entries are very similar; only the first and the last residue differ from the previous and the next data entry. Shuffling the whole training data diversifies the input in each batch. We experimentally trained the model with and without data shuffling, and shuffling the data rendered better predictions. 

\subsection{Parameter tuning}
Parameters and hyper-parameters are chosen based on the training dataset while applying early stopping \citep{prechelt1998early} on the validation set. Early stopping halts the training process when a performance drop on the validation set is detected. This is to avoid overfitting the training dataset. We chose all parameters with the purpose to maximize area under the precision-recall curve (AUPRC) of the training data. All testing results are then carried using the already tuned model. All parameters and hyper-parameters used in this model are shown in Table \ref{tab_parameter}. 

The DELPHI model takes 2.1 hours to train the CNN component, 0.5 hour to train the RNN component and 1.3 hours to train the ensemble model on our testing cluster.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{Parameters used in DELPHI. Parameters are divided into four groups: CNN, RNN, ensemble model, and hyper-parameters.}
    \begin{tabular}{lr}
    Parameter & Value \\
    \midrule
    \midrule
    Epoch in CNN & 8 \\
    Kernel size in CNN & 5 \\
    Stride in CNN & 1 \\
    Padding in CNN & valid \\
    Number of filters in CNN & 64 \\
    Fully connected unit in CNN & 64, 1 \\
    \midrule
    Epoch in RNN & 9 \\
    GRU unit & 32 \\
    Fully connected unit in RNN & 64, 1 \\
    \midrule
    Epoch in ensemble & 5 \\
    Fully connected unit in ensemble & 96, 1 \\
    \midrule
    Batch size & 1024 \\
    Dropout rate & 0.3 \\
    Optimizer & Adam ($\beta_1=0.9, \beta_2=0.999$) \\
    Patience in early stop & 4 \\
    Loss function & binary cross entropy \\
    Learning rate & 0.002 \\
    \bottomrule
    \end{tabular}%
  \label{tab_parameter}%
\end{table}%

% \enlargethispage{6pt}
\end{methods}

\section{Results}
\subsection{Competing methods}
We have comprehensively compared DELPHI with nine state-of-the-art machine learning based methods. The methods are selected using the following criteria. First, the program is a sequence-based method as sequence information is readily available for most proteins. Second, the program is available in the form of source code or web server. Lastly, the program takes in any input sequence in FASTA format and produces the results on an average-length protein within thirty minutes. Following these criteria, DLpred \citep{zhang2019sequence}, SCRIBER \citep{zhang2019scriber}, SSWRF \citep{wei2016protein}, SPRINT \citep{taherzadeh2016sequence}, CRF-PPI \citep{wei2015cascade}, LORIS \citep{dhole2014sequence}, SPRINGS \citep{singh2014springs}, PSIVER \citep{murakami2010applying}, and SPPIDER \citep{porollo2007prediction} are selected.

All competing methods are pre-trained using their own training and validation datasets. The most revent two programs, DLPred and SCRIBER, use 5719 and 843 training proteins respectively. The training dataset of DLPred is obtained from CullPDB datasets \citep{wang2003pisces} and further filtered by the authors. The SCRIBER training dataset is originally from the BioLip database. This dataset contains also protein binding information with DNA, RNA, and ligand, which is used by SCRIBER.

All tests have been performed on a Linux (Ubuntu 16.04) machine with 24 CPUs (Intel Xeon v4, 3.00GHz), 256GB memory, and a Nvidia Tesla K40c GPU.

\subsection{Evaluation scheme}
Similar to previous studies, we use sensitivity, specificity, precision, accuracy, F1-measure, (F1), Matthews correlation coefficient (MCC), area under the receiver operating characteristic curve (AUROC), and area under the precision-recall (AUPRC) to measure the prediction performance. All programs output a prediction value for each amino acid, and thus the receiver operating characteristic (ROC) curve and the precision-recall (PR) curve can be drawn. AUROC and AUPRC are computed based on the curves using Scikit-learn~\citep{scikit-learn}. We focus more on AUROC and AUPR because they are threshold independent and convey an overall performance measurement of a program. The rest of the metrics are calculated using a binding threshold which is determined after obtaining the prediction scores from each program. Since each program's output is of different scale, for each program, we pick the threshold such that for a given testing dataset, the number of predicted scores above the threshold is equal to the real number of binding sites in the dataset. 

The formulas for calculating the metrics are as follows, where true positives ($TP$) and true negatives ($TN$) are the correctly predicted binding sites and non-binding sites, respectively, and false positives ($FP$) and false negative ($FN$) are incorrectly predicted binding sites and non-binding sites, respectively.
\begin{equation}
\textit{Sensitivity} = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}
\textit{Specificity} = \frac{TN}{TN+FP}
\end{equation}

\begin{equation}
\textit{Precision} = \frac{TP}{TP + FP} 
\end{equation}

\begin{equation}
\textit{Accuracy}=\frac{TP+TN}{TP+FN+TN+FP}
\end{equation}

\begin{equation}
F1=2\times \frac{\textit{Sensitivity}\times \textit{Precision}}{\textit{Sensitivity}+\textit{Precision}}
\end{equation}

\begin{equation}
MCC\!=\!\frac{TP \times TN - FN \times FP}{\sqrt{(TP\!+\!FP)\!\times\! (TP\!+\!FN)\! \times \!(TN\!+\!FP)\!\times\!(TN\!+\!FN)}}
\end{equation}

\subsection{Comparative assessment of predictive performance}
\subsubsection{Performance comparison on Dset\_448}
We first compare the DELPHI model with eight programs on Dset\_448. This dataset is the most recently published and has the largest number of proteins, so we emphasize the importance of this dataset. As shown in Table~\ref{tab_comp_448}, DELPHI surpasses competitors in all metrics with an improvement of 3.08\% and 17.4\% on AUROC and AUPR respectively comparing to the second best program SCRIBER.

\begin{table*}[htbp]
  \centering
  \caption{Performance comparison of nine programs on Dset\_448. Programs are sorted in asending order by AUPRC. Darker colours represent better results. The evaluation of the programs marked with \** are carried by \citep{zhang2019scriber}.}
    % Table generated by Excel2LaTeX from sheet 'Sheet2'

    \begin{tabular}{lrrrrrrrr}
    \toprule
    \multicolumn{1}{|l}{Predictor} & \multicolumn{1}{c}{Sensitivity} & \multicolumn{1}{c}{Specificity} & \multicolumn{1}{c}{Precision} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{MCC} & \multicolumn{1}{c}{AUROC} & \multicolumn{1}{c|}{AUPRC} \\
    \midrule
    \multicolumn{9}{|c|}{Dset\_448} \\
    \midrule
    SPPIDER* & \cellcolor[rgb]{ .933,  .953,  .922}0.202 & 0.870 & \cellcolor[rgb]{ .961,  .973,  .957}0.194 & 0.781 & \cellcolor[rgb]{ .949,  .965,  .937}0.198 & \cellcolor[rgb]{ .957,  .969,  .949}0.071 & 0.517 & 0.159 \\
    SPRINT* & 0.183 & \cellcolor[rgb]{ .937,  .953,  .925}0.873 & 0.183 & 0.781 & 0.183 & 0.057 & \cellcolor[rgb]{ .839,  .882,  .812}0.570 & \cellcolor[rgb]{ .973,  .98,  .965}0.167 \\
    PSIVER* & \cellcolor[rgb]{ .973,  .98,  .969}0.191 & \cellcolor[rgb]{ .918,  .937,  .902}0.874 & \cellcolor[rgb]{ .973,  .98,  .969}0.191 & \cellcolor[rgb]{ .973,  .98,  .969}0.783 & \cellcolor[rgb]{ .973,  .98,  .969}0.191 & \cellcolor[rgb]{ .973,  .98,  .969}0.066 & \cellcolor[rgb]{ .808,  .859,  .773}0.581 & \cellcolor[rgb]{ .961,  .973,  .953}0.170 \\
    SPRINGS* & \cellcolor[rgb]{ .839,  .882,  .808}0.229 & \cellcolor[rgb]{ .745,  .812,  .698}0.882 & \cellcolor[rgb]{ .843,  .886,  .812}0.228 & \cellcolor[rgb]{ .792,  .847,  .753}0.796 & \cellcolor[rgb]{ .839,  .882,  .808}0.229 & \cellcolor[rgb]{ .835,  .878,  .804}0.111 & \cellcolor[rgb]{ .675,  .761,  .612}0.625 & \cellcolor[rgb]{ .843,  .886,  .816}0.201 \\
    LORIS* & \cellcolor[rgb]{ .714,  .792,  .659}0.264 & \cellcolor[rgb]{ .635,  .733,  .569}0.887 & \cellcolor[rgb]{ .718,  .792,  .663}0.263 & \cellcolor[rgb]{ .667,  .757,  .604}0.805 & \cellcolor[rgb]{ .718,  .792,  .663}0.263 & \cellcolor[rgb]{ .71,  .788,  .655}0.151 & \cellcolor[rgb]{ .58,  .694,  .502}0.656 & \cellcolor[rgb]{ .741,  .812,  .694}0.228 \\
    CRFPPI* & \cellcolor[rgb]{ .698,  .78,  .643}0.268 & \cellcolor[rgb]{ .635,  .733,  .569}0.887 & \cellcolor[rgb]{ .714,  .792,  .659}0.264 & \cellcolor[rgb]{ .667,  .757,  .604}0.805 & \cellcolor[rgb]{ .706,  .784,  .651}0.266 & \cellcolor[rgb]{ .698,  .78,  .643}0.154 & \cellcolor[rgb]{ .502,  .635,  .412}0.681 & \cellcolor[rgb]{ .706,  .784,  .651}0.238 \\
    SSWRF* & \cellcolor[rgb]{ .627,  .729,  .561}0.288 & \cellcolor[rgb]{ .549,  .671,  .471}0.891 & \cellcolor[rgb]{ .635,  .733,  .569}0.286 & \cellcolor[rgb]{ .584,  .694,  .506}0.811 & \cellcolor[rgb]{ .631,  .729,  .565}0.287 & \cellcolor[rgb]{ .624,  .725,  .557}0.178 & \cellcolor[rgb]{ .482,  .624,  .388}0.687 & \cellcolor[rgb]{ .639,  .737,  .573}0.256 \\
    SCRIBER & \cellcolor[rgb]{ .463,  .608,  .365}0.334 & \cellcolor[rgb]{ .443,  .592,  .341}0.896 & \cellcolor[rgb]{ .471,  .612,  .373}0.332 & \cellcolor[rgb]{ .443,  .592,  .341}0.821 & \cellcolor[rgb]{ .467,  .612,  .369}0.333 & \cellcolor[rgb]{ .463,  .608,  .365}0.230 & \cellcolor[rgb]{ .4,  .561,  .29}0.715 & \cellcolor[rgb]{ .522,  .651,  .435}0.287 \\
    DELPHI & \cellcolor[rgb]{ .329,  .51,  .208}0.371 & \cellcolor[rgb]{ .329,  .51,  .208}0.901 & \cellcolor[rgb]{ .329,  .51,  .208}0.371 & \cellcolor[rgb]{ .329,  .51,  .208}0.829 & \cellcolor[rgb]{ .329,  .51,  .208}0.371 & \cellcolor[rgb]{ .329,  .51,  .208}0.272 & \cellcolor[rgb]{ .329,  .51,  .208}0.737 & \cellcolor[rgb]{ .329,  .51,  .208}0.337 \\
    \end{tabular}%
  \label{tab_comp_448}%
\end{table*}%

\begin{table*}
  \centering
  \caption{Performance comparison of SCRIBER, DLPred, and DELPHI on Dset\_355, Dset\_186, Dset\_164, and Dset\_72. Darker colours represent better results. Each dataset is tested separately using the same metrics. The average metrics of the three datasets is also shown.}
    \begin{tabular}{|lrrrrrrrr|}
    \toprule
    Predictor & \multicolumn{1}{c}{Sensitivity} & \multicolumn{1}{c}{Specificity} & \multicolumn{1}{c}{Precision} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{F1} & \multicolumn{1}{c}{MCC} & \multicolumn{1}{c}{AUROC} & \multicolumn{1}{c|}{AUPRC} \\
    \midrule
    \multicolumn{9}{|c|}{Dset\_355} \\
    \midrule
    SCRIBER & \cellcolor[rgb]{ .835,  .882,  .808}0.322 & \cellcolor[rgb]{ .847,  .886,  .816}0.908 & \cellcolor[rgb]{ .839,  .882,  .808}0.322 & \cellcolor[rgb]{ .847,  .89,  .82}0.838 & \cellcolor[rgb]{ .839,  .882,  .808}0.322 & \cellcolor[rgb]{ .839,  .882,  .808}0.230 & 0.719 & \cellcolor[rgb]{ .961,  .973,  .953}0.275 \\
    DLPred & 0.308 & 0.906 & 0.308 & 0.835 & 0.308 & 0.214 & \cellcolor[rgb]{ .89,  .922,  .871}0.724 & 0.272 \\
    DELPHI & \cellcolor[rgb]{ .329,  .51,  .208}0.364 & \cellcolor[rgb]{ .329,  .51,  .208}0.914 & \cellcolor[rgb]{ .329,  .51,  .208}0.364 & \cellcolor[rgb]{ .329,  .51,  .208}0.848 & \cellcolor[rgb]{ .329,  .51,  .208}0.364 & \cellcolor[rgb]{ .329,  .51,  .208}0.278 & \cellcolor[rgb]{ .329,  .51,  .208}0.746 & \cellcolor[rgb]{ .329,  .51,  .208}0.326 \\
    \midrule
    \multicolumn{9}{|c|}{Dset\_186} \\
    \midrule
    SCRIBER & 0.279 & 0.870 & 0.279 & 0.780 & 0.279 & 0.150 & 0.647 & 0.246 \\
    DLPred & \cellcolor[rgb]{ .62,  .722,  .549}0.320 & \cellcolor[rgb]{ .631,  .729,  .565}0.878 & \cellcolor[rgb]{ .62,  .722,  .549}0.320 & \cellcolor[rgb]{ .631,  .729,  .561}0.793 & \cellcolor[rgb]{ .62,  .722,  .549}0.320 & \cellcolor[rgb]{ .62,  .722,  .553}0.198 & \cellcolor[rgb]{ .498,  .631,  .404}0.694 & \cellcolor[rgb]{ .6,  .71,  .529}0.290 \\
    DELPHI & \cellcolor[rgb]{ .329,  .51,  .208}0.351 & \cellcolor[rgb]{ .329,  .51,  .208}0.884 & \cellcolor[rgb]{ .329,  .51,  .208}0.351 & \cellcolor[rgb]{ .329,  .51,  .208}0.803 & \cellcolor[rgb]{ .329,  .51,  .208}0.351 & \cellcolor[rgb]{ .329,  .51,  .208}0.235 & \cellcolor[rgb]{ .329,  .51,  .208}0.710 & \cellcolor[rgb]{ .329,  .51,  .208}0.319 \\
    \midrule
    \multicolumn{9}{|c|}{Dset\_164} \\
    \midrule
    SCRIBER & 0.327 & 0.851 & 0.327 & 0.756 & 0.327 & 0.179 & 0.657 & 0.301 \\
    DLPred & \cellcolor[rgb]{ .714,  .792,  .663}0.338 & \cellcolor[rgb]{ .71,  .788,  .659}0.854 & \cellcolor[rgb]{ .714,  .792,  .663}0.338 & \cellcolor[rgb]{ .714,  .788,  .659}0.760 & \cellcolor[rgb]{ .714,  .792,  .663}0.338 & \cellcolor[rgb]{ .714,  .792,  .663}0.192 & \cellcolor[rgb]{ .635,  .733,  .569}0.672 & \cellcolor[rgb]{ .365,  .537,  .251}0.330 \\
    DELPHI & \cellcolor[rgb]{ .329,  .51,  .208}0.352 & \cellcolor[rgb]{ .329,  .51,  .208}0.857 & \cellcolor[rgb]{ .329,  .51,  .208}0.352 & \cellcolor[rgb]{ .329,  .51,  .208}0.765 & \cellcolor[rgb]{ .329,  .51,  .208}0.352 & \cellcolor[rgb]{ .329,  .51,  .208}0.209 & \cellcolor[rgb]{ .329,  .51,  .208}0.685 & \cellcolor[rgb]{ .329,  .51,  .208}0.332 \\
    \midrule
    \multicolumn{9}{|c|}{Dset\_72} \\
    \midrule
    SCRIBER & 0.232 & \cellcolor[rgb]{ .612,  .718,  .541}0.909 & 0.232 & \cellcolor[rgb]{ .635,  .733,  .569}0.837 & 0.232 & 0.141 & 0.680 & 0.198 \\
    DLPred & \cellcolor[rgb]{ .776,  .839,  .737}0.246 & 0.901 & \cellcolor[rgb]{ .776,  .839,  .737}0.246 & 0.826 & \cellcolor[rgb]{ .776,  .839,  .737}0.246 & \cellcolor[rgb]{ .91,  .937,  .894}0.148 & \cellcolor[rgb]{ .82,  .867,  .784}0.688 & \cellcolor[rgb]{ .706,  .784,  .651}0.215 \\
    DELPHI & \cellcolor[rgb]{ .329,  .51,  .208}0.274 & \cellcolor[rgb]{ .329,  .51,  .208}0.914 & \cellcolor[rgb]{ .329,  .51,  .208}0.274 & \cellcolor[rgb]{ .329,  .51,  .208}0.847 & \cellcolor[rgb]{ .329,  .51,  .208}0.274 & \cellcolor[rgb]{ .329,  .51,  .208}0.189 & \cellcolor[rgb]{ .329,  .51,  .208}0.711 & \cellcolor[rgb]{ .329,  .51,  .208}0.237 \\
    \midrule
    \multicolumn{9}{|c|}{Average} \\
    \midrule
    SCRIBER & 0.290 & 0.885 & 0.290 & 0.803 & 0.290 & 0.175 & 0.676 & 0.255 \\
    DLPred & \cellcolor[rgb]{ .812,  .863,  .776}0.303 & \cellcolor[rgb]{ .992,  .992,  .988}0.885 & \cellcolor[rgb]{ .808,  .859,  .773}0.303 & \cellcolor[rgb]{ .976,  .984,  .973}0.803 & \cellcolor[rgb]{ .808,  .863,  .773}0.303 & \cellcolor[rgb]{ .835,  .882,  .804}0.188 & \cellcolor[rgb]{ .659,  .753,  .6}0.695 & \cellcolor[rgb]{ .702,  .784,  .647}0.277 \\
    DELPHI & \cellcolor[rgb]{ .329,  .51,  .208}0.335 & \cellcolor[rgb]{ .329,  .51,  .208}0.892 & \cellcolor[rgb]{ .329,  .51,  .208}0.335 & \cellcolor[rgb]{ .329,  .51,  .208}0.816 & \cellcolor[rgb]{ .329,  .51,  .208}0.335 & \cellcolor[rgb]{ .329,  .51,  .208}0.227 & \cellcolor[rgb]{ .329,  .51,  .208}0.713 & \cellcolor[rgb]{ .329,  .51,  .208}0.304 \\
    \bottomrule\label{tab_ds355_ds186_164_72}
    \end{tabular}%
\end{table*}%

In order to have a more comprehensive comparison, we compare DELPHI with another recent sequence-based program, DLPred. However, the training data in DLPred has a big overlap with Dset\_448, so we removed the highly similar sequence in Dset\_448 using the protocol described in section \ref{testing_data} and produced a reduced dataset Dset\_355 which is tested in Section \ref{section_four_tests}. 

\subsubsection{Performance comparison on Dset\_355, Dset\_186, Dset\_164, and Dset\_72}\label{section_four_tests}
To further compare DELPHI with other programs, we used Dset\_355 along with another three previously published datasets: Dset\_186, Dset\_164, and Dset\_72. Since DLPred and SCRIBER are the most recent programs and have been shown to outperform older programs, we compare DELPHI only with SCRIBER and DLPred on the remaining datasets. As shown in Table \ref{tab_ds355_ds186_164_72}, DELPHI clearly outperforms the competitors in all metrics on all datasets it shares the least similarities to the testing datasets. As binding site prediction is a highly imbalanced task, the area under the PR curve is a better indication of the performance as it emphasises more on the minority class and ROC curve pays attention to both minority and the majority class \citep{andluis2016survey}. The AUPR is improved by 18.5\%, 10.0\%, 0.6\%, 10.2\% comparing to the second best program on each dataset. We present also the average values over Dset\_355, Dset\_186, Dset\_164, and Dset\_72 in Table \ref{tab_ds355_ds186_164_72} for each specificity value. The performance of DELPHI again surpasses all competing programs. The average AUPR improvement is 9.7\%.  

\subsubsection{Feature evaluation}
We conducted an another experiment to show that all twelve feature are necessary for DELPHI. We pruned one feature each time, and the remaining eleven features are used to train and then evaluate the DELPHI model. As shown in Fig. \ref{fig_remove_each_feature}, the performance decreases with the removal of any feature, showing that there are no redundant features. It is perhaps expected that  removing PSSM creates the biggest performance drop, but our newly introduced features, HSP, ProtVec1D, and Position are shown to be  very useful as well. 
%\textit{First, we carried an experiment to show that each individual feature is useful by itself. The model is trained using each individual feature only and tested on Dset\_448. The binding residual ratio of Dset\_448 is 13.57\%, so an AUPR value greater than that indicates a feature providing valid information if used alone. %As shown in Fig. \ref{fig_each_feature}, each individual feature achieved 13.57\%. Among the twelve used features, PSSM, position, and HSP achieved the top 3 scores which means they provide the most information related to protein binding if used alone.Second,} 
% \begin{figure}
% \centering
% \includegraphics[width=8cm,height=8cm]{img/each_features.pdf}
%   \caption{\textbf{The area under PR curve of using each individual feature to train the program on Dset\_448.} The DELPHI model is trained, validated, and tested the same way using one feature at a time. The features are sorted by its achieved AUPR.  
%   \label{fig_each_feature}}
% \end{figure}

\begin{figure}
\centering
\includegraphics[width=8cm,height=4cm]{remove_features_individually_Testing.pdf}
  \caption{\textbf{The areas under PR curves with the removal of one out of the twelve features on Dset\_448.} One feature is removed each time, and the DELPHI model is trained, validated, and tested using the remaining eleven features. The x-axis shows the removed features where 'None' indicates using all twelve features, and the y-axis is the AUPR achieved. The features are sorted by the AUPR values. 
  \label{fig_remove_each_feature}}
\end{figure}

\subsubsection{Other explorations}
We also tried some other ideas that could be potentially interesting. To represent the protein sequence, we attempted to use one-hot embedding with various out dimensions. The original 100 dimensional ProtVec is also tried. Unfortunately the results did not surpass the existing sequence representation. Under-sampling non-binding residuals and over-sampling binding residuals are also tried. However, the performance is unstable and related to the ratio of the testing datasets. For example the prediction on the highest binding residue ratio Dset\_164 was improved by the sampling methods but not on other datasets. Attention mechanism has also been tried to improve the performance. However, we did not obtain better results. In terms of the network architecture, LSTM, 1D convolution, and combining the CNN and RNN network without training them separately have been tested. In the end we picked the reported architecture empirically. 

\subsection{Software availability}
DELPHI is an open source program under the GPLv3 License. The trained model, source code of training, predicting, and data processing is freely available at \texttt{https://github.com/lucian-ilie/DELPHI}. 
All datasets used in this study can be downloaded at\\
\texttt{http://www.csd.uwo.ca/\~{}ilie/DELPHI/}.\\

\section{Conclusion}
We have presented a new deep learning model and program DELPHI, for predicting PPI binding sites. We compared DELPHI with nine current state-of-the-art programs on five datasets and demonstrated that DELPHI has a higher prediction performance. There is still plenty room for improvement on this topic as the highest AUROC in all test is 0.746. 
We hope in the future, the model architecture, the usage of the three new features, and the many-to-one structure can be extended to predicting protein biding with other types of molecules, such as DNA, RNA, and ligand. Other deep learning techniques could be better used on this topic including better pre-trained embedding, better sampling methods for imbalanced problems in bioinformatics. 

% \begin{figure*}
% \centering
% \includegraphics[width=\textwidth]{img/combine_plots.pdf}
%   \caption{The PR curves (top row) and ROC curves (bottom row) for Dset\_355, Dset\_186, Dset\_164, and Dset\_72, from left to right.}
%   \label{fig_ROC_PR}
% \end{figure*}
\section*{Acknowledgements}
We would like to thank Jian Zhang and Buzhong Zhang for running their programs on some of the testing datasets as well as the insightful information they provided regarding SCRIBER and DLPred, Giancarlo Colmenare for suggestions on tuning the network, Min Zeng for the explanation on DeepPPISP, and Tanner Bohn for suggesting using the position information.

\section*{Funding}
This work was supported by a Discovery Grant (R3143A01) and a Research Tools and Instruments Grant (R3143A07) from the Natural Sciences and Engineering Research Council of Canada.

%\bibliographystyle{natbib}
%\bibliographystyle{achemnat}
%\bibliographystyle{plainnat}
%\bibliographystyle{abbrv}
% \bibliographystyle{bioinformatics}
\bibliographystyle{plain}
%  \clearpage
 \bibliography{reference}
\end{document}

